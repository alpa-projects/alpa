Init model
Total size:  0.50 GB
Train
HloModule xmap_train_step.57

%add (x: f32[], y: f32[]) -> f32[] {
  %x = f32[] parameter(0)
  %y = f32[] parameter(1)
  ROOT %add = f32[] add(f32[] %x, f32[] %y)
}

%fused_computation (param_1: f32[2048,8192], param_1.1: f32[2048,8192], param_2.2: f32[4,2048,2048]) -> f32[2048,8192] {
  %param_1 = f32[2048,8192]{1,0} parameter(0)
  %param_1.1 = f32[2048,8192]{1,0} parameter(1)
  %param_2.2 = f32[4,2048,2048]{2,0,1} parameter(2)
  %transpose.1 = f32[2048,4,2048]{2,1,0} transpose(f32[4,2048,2048]{2,0,1} %param_2.2), dimensions={1,0,2}
  %bitcast.1 = f32[2048,8192]{1,0} bitcast(f32[2048,4,2048]{2,1,0} %transpose.1)
  %multiply.5 = f32[2048,8192]{1,0} multiply(f32[2048,8192]{1,0} %param_1.1, f32[2048,8192]{1,0} %bitcast.1), metadata={op_type="mul" op_name="xmap(train_step)/mul" source_file="/home/ubuntu/anaconda3/lib/python3.7/site-packages/flax/optim/sgd.py" source_line=45}
  ROOT %subtract.3 = f32[2048,8192]{1,0} subtract(f32[2048,8192]{1,0} %param_1, f32[2048,8192]{1,0} %multiply.5), metadata={op_type="sub" op_name="xmap(train_step)/sub" source_file="/home/ubuntu/anaconda3/lib/python3.7/site-packages/flax/optim/sgd.py" source_line=45}
}

%fused_computation.1 (param_0.2: f32[2048,2048], param_1.7: f32[2048,2048]) -> f32[2048,2048] {
  %param_1.7 = f32[2048,2048]{1,0} parameter(1)
  %constant_11 = f32[] constant(0), metadata={op_type="max" op_name="xmap(train_step)/custom_jvp_call_jaxpr/max" source_file="train.py" source_line=31}
  %broadcast.7 = f32[2048,2048]{1,0} broadcast(f32[] %constant_11), dimensions={}, metadata={op_type="gt" op_name="xmap(train_step)/gt" source_file="train.py" source_line=31}
  %compare.1 = pred[2048,2048]{1,0} compare(f32[2048,2048]{1,0} %param_1.7, f32[2048,2048]{1,0} %broadcast.7), direction=GT, metadata={op_type="gt" op_name="xmap(train_step)/gt" source_file="train.py" source_line=31}
  %param_0.2 = f32[2048,2048]{1,0} parameter(0)
  ROOT %select.1 = f32[2048,2048]{1,0} select(pred[2048,2048]{1,0} %compare.1, f32[2048,2048]{1,0} %param_0.2, f32[2048,2048]{1,0} %broadcast.7), metadata={op_type="select" op_name="xmap(train_step)/select" source_file="train.py" source_line=31}
}

%fused_computation.2 (param_0.4: f32[2048,8192], param_1.6: f32[2048,8192]) -> f32[2048,8192] {
  %param_0.4 = f32[2048,8192]{1,0} parameter(0)
  %param_1.6 = f32[2048,8192]{1,0} parameter(1)
  %subtract.4 = f32[2048,8192]{1,0} subtract(f32[2048,8192]{1,0} %param_0.4, f32[2048,8192]{1,0} %param_1.6), metadata={op_type="sub" op_name="xmap(train_step)/sub" source_file="train.py" source_line=50}
  %constant_10 = f32[] constant(1.1920929e-07)
  %broadcast.6 = f32[2048,8192]{1,0} broadcast(f32[] %constant_10), dimensions={}
  ROOT %multiply.6 = f32[2048,8192]{1,0} multiply(f32[2048,8192]{1,0} %subtract.4, f32[2048,8192]{1,0} %broadcast.6), metadata={op_type="mul" op_name="xmap(train_step)/mul" source_file="train.py" source_line=50}
}

%fused_computation.3 (param_0.5: f32[2048,2048]) -> f32[2048,2048] {
  %param_0.5 = f32[2048,2048]{1,0} parameter(0)
  %constant_12 = f32[] constant(0), metadata={op_type="max" op_name="xmap(train_step)/custom_jvp_call_jaxpr/max" source_file="train.py" source_line=31}
  %broadcast.8 = f32[2048,2048]{1,0} broadcast(f32[] %constant_12), dimensions={}, metadata={op_type="gt" op_name="xmap(train_step)/gt" source_file="train.py" source_line=31}
  ROOT %maximum.1 = f32[2048,2048]{1,0} maximum(f32[2048,2048]{1,0} %param_0.5, f32[2048,2048]{1,0} %broadcast.8), metadata={op_type="max" op_name="xmap(train_step)/custom_jvp_call_jaxpr/max" source_file="train.py" source_line=31}
}

ENTRY %xmap_train_step.57_spmd (param: s32[], param.1: f32[8192,2048], param.3: f32[2048,8192], param.2: f32[2048,8192], param.4: f32[2048,8192]) -> (s32[], f32[8192,2048], f32[2048,8192]) {
  %param = s32[] parameter(0), parameter_replication={false}, sharding={replicated}
  %constant_2 = s32[] constant(1), metadata={op_type="add" op_name="xmap(train_step)/add" source_file="/home/ubuntu/anaconda3/lib/python3.7/site-packages/flax/optim/base.py" source_line=93}
  %add.1 = s32[] add(s32[] %param, s32[] %constant_2), metadata={op_type="add" op_name="xmap(train_step)/add" source_file="/home/ubuntu/anaconda3/lib/python3.7/site-packages/flax/optim/base.py" source_line=93}
  %param.1 = f32[8192,2048]{1,0} parameter(1), parameter_replication={false}, sharding={devices=[1,4]0,1,2,3}
  %param.2 = f32[2048,8192]{1,0} parameter(3), parameter_replication={false}, sharding={replicated}
  %custom-call = f32[2048,2048]{1,0} custom-call(f32[2048,8192]{1,0} %param.2, f32[8192,2048]{1,0} %param.1), custom_call_target="__cublas$gemm", metadata={op_type="dot_general" op_name="xmap(train_step)/dot_general[ dimension_numbers=(((1,), (0,)), ((), ()))\n                              precision=None\n                              preferred_element_type=None ]" source_file="/home/ubuntu/anaconda3/lib/python3.7/site-packages/flax/linen/linear.py" source_line=176}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"-1\"}"
  %fusion.3 = f32[2048,2048]{1,0} fusion(f32[2048,2048]{1,0} %custom-call), kind=kLoop, calls=%fused_computation.3, metadata={op_type="max" op_name="xmap(train_step)/custom_jvp_call_jaxpr/max" source_file="train.py" source_line=31}
  %param.3 = f32[2048,8192]{1,0} parameter(2), parameter_replication={false}, sharding={devices=[4,1]0,1,2,3}
  %custom-call.1 = f32[2048,8192]{1,0} custom-call(f32[2048,2048]{1,0} %fusion.3, f32[2048,8192]{1,0} %param.3), custom_call_target="__cublas$gemm", backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"-1\"}"
  %all-reduce = f32[2048,8192]{1,0} all-reduce(f32[2048,8192]{1,0} %custom-call.1), channel_id=1, replica_groups={{0}}, to_apply=%add, metadata={op_type="dot_general" op_name="xmap(train_step)/dot_general[ dimension_numbers=(((1,), (0,)), ((), ()))\n                              precision=None\n                              preferred_element_type=None ]" source_file="/home/ubuntu/anaconda3/lib/python3.7/site-packages/flax/linen/linear.py" source_line=176}
  %param.4 = f32[2048,8192]{1,0} parameter(4), parameter_replication={false}, sharding={replicated}
  %fusion.2 = f32[2048,8192]{1,0} fusion(f32[2048,8192]{1,0} %all-reduce, f32[2048,8192]{1,0} %param.4), kind=kLoop, calls=%fused_computation.2, metadata={op_type="mul" op_name="xmap(train_step)/mul" source_file="train.py" source_line=50}
  %custom-call.2 = f32[2048,2048]{1,0} custom-call(f32[2048,8192]{1,0} %fusion.2, f32[2048,8192]{1,0} %param.3), custom_call_target="__cublas$gemm", metadata={op_type="dot_general" op_name="xmap(train_step)/dot_general[ dimension_numbers=(((1,), (1,)), ((), ()))\n                              precision=None\n                              preferred_element_type=None ]" source_file="/home/ubuntu/anaconda3/lib/python3.7/site-packages/flax/linen/linear.py" source_line=176}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"1\"],\"rhs_contracting_dimensions\":[\"1\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"-1\"}"
  %fusion.1 = f32[2048,2048]{1,0} fusion(f32[2048,2048]{1,0} %custom-call.2, f32[2048,2048]{1,0} %custom-call), kind=kLoop, calls=%fused_computation.1, metadata={op_type="select" op_name="xmap(train_step)/select" source_file="train.py" source_line=31}
  %custom-call.3 = f32[8192,2048]{1,0} custom-call(f32[2048,8192]{1,0} %param.2, f32[2048,2048]{1,0} %fusion.1), custom_call_target="__cublas$gemm", metadata={op_type="transpose" op_name="xmap(train_step)/transpose[ permutation=(1, 0) ]" source_file="/home/ubuntu/anaconda3/lib/python3.7/site-packages/flax/linen/linear.py" source_line=176}, backend_config="{\"alpha_real\":0.0099999997764825821,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"-1\"}"
  %subtract.1 = f32[8192,2048]{1,0} subtract(f32[8192,2048]{1,0} %param.1, f32[8192,2048]{1,0} %custom-call.3), metadata={op_type="sub" op_name="xmap(train_step)/sub" source_file="/home/ubuntu/anaconda3/lib/python3.7/site-packages/flax/optim/sgd.py" source_line=45}
  %custom-call.4 = f32[2048,8192]{1,0} custom-call(f32[2048,2048]{1,0} %fusion.3, f32[2048,8192]{1,0} %fusion.2), custom_call_target="__cublas$gemm", metadata={op_type="transpose" op_name="xmap(train_step)/transpose[ permutation=(1, 0) ]" source_file="/home/ubuntu/anaconda3/lib/python3.7/site-packages/flax/linen/linear.py" source_line=176}, backend_config="{\"alpha_real\":1,\"alpha_imag\":0,\"beta\":0,\"dot_dimension_numbers\":{\"lhs_contracting_dimensions\":[\"0\"],\"rhs_contracting_dimensions\":[\"0\"],\"lhs_batch_dimensions\":[],\"rhs_batch_dimensions\":[]},\"batch_size\":\"1\",\"selected_algorithm\":\"-1\"}"
  %constant_7 = f32[] constant(0.01), metadata={op_type="mul" op_name="xmap(train_step)/mul" source_file="/home/ubuntu/anaconda3/lib/python3.7/site-packages/flax/optim/sgd.py" source_line=45}
  %broadcast.4 = f32[4,2048,2048]{2,0,1} broadcast(f32[] %constant_7), dimensions={}
  %all-to-all = f32[4,2048,2048]{2,0,1} all-to-all(f32[4,2048,2048]{2,0,1} %broadcast.4), channel_id=2, replica_groups={{0,1,2,3}}, dimensions={0}
  %fusion = f32[2048,8192]{1,0} fusion(f32[2048,8192]{1,0} %param.3, f32[2048,8192]{1,0} %custom-call.4, f32[4,2048,2048]{2,0,1} %all-to-all), kind=kLoop, calls=%fused_computation, metadata={op_type="sub" op_name="xmap(train_step)/sub" source_file="/home/ubuntu/anaconda3/lib/python3.7/site-packages/flax/optim/sgd.py" source_line=45}
  ROOT %tuple = (s32[], f32[8192,2048]{1,0}, f32[2048,8192]{1,0}) tuple(s32[] %add.1, f32[8192,2048]{1,0} %subtract.1, f32[2048,8192]{1,0} %fusion)
}


idx: 72/81, cost : 222130339.84
Best idx: 72
Best cost: 222130339.84
Best in_axes: (OrderedDict(), OrderedDict([('mesh_x', 1)]), OrderedDict([('mesh_x', 0)]), OrderedDict(), OrderedDict())
[162.32657433 147.33552933 148.02742004 148.39696884 147.48859406
 147.78494835 147.47190475 147.4609375 ]
Mean cost: 147.64 ms (std: 4.85 ms)
