{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yonghao.zhuang/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:__main__:Training/evaluation parameters TrainingArguments(output_dir='./output', overwrite_output_dir=False, do_train=True, do_eval=False, per_device_train_batch_size=8, per_device_eval_batch_size=8, num_micro_batches=1, operator_parallel=1, pipeline_parallel=1, use_remat=True, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, adafactor=False, num_train_epochs=3.0, warmup_ratio=0.0, logging_steps=500, save_steps=500, eval_steps=None, seed=42, push_to_hub=False, hub_model_id=None, hub_token=None)\n",
      "Model config LLaMAConfig {\n",
      "  \"attn_pdrop\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"fcm_max_ratio\": 0.0,\n",
      "  \"fcm_min_ratio\": 0.0,\n",
      "  \"gradient_checkpointing\": \"nothing_saveable\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_sequence_length\": 2048,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file tokenizer.model\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StreamExecutorGpuDevice(id=0, process_index=0), StreamExecutorGpuDevice(id=1, process_index=0), StreamExecutorGpuDevice(id=2, process_index=0), StreamExecutorGpuDevice(id=3, process_index=0), StreamExecutorGpuDevice(id=4, process_index=0), StreamExecutorGpuDevice(id=5, process_index=0), StreamExecutorGpuDevice(id=6, process_index=0), StreamExecutorGpuDevice(id=7, process_index=0)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"transformers_version\": \"4.28.1\"\n",
      "}\n",
      "\n",
      "2023-05-13 07:31:56.335021: E external/org_tensorflow/tensorflow/compiler/xla/pjrt/pjrt_stream_executor_client.cc:2156] Execution of replica 0 failed: INTERNAL: CustomCall failed: jaxlib/cuda/cuda_prng_kernels.cc:32: operation cudaGetLastError() failed: out of memory\n"
     ]
    },
    {
     "ename": "XlaRuntimeError",
     "evalue": "INTERNAL: CustomCall failed: jaxlib/cuda/cuda_prng_kernels.cc:32: operation cudaGetLastError() failed: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXlaRuntimeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 452\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[39m# Monkey patch the model's init to init_dummy\u001b[39;00m\n\u001b[1;32m    451\u001b[0m do_monkey_patch()\n\u001b[0;32m--> 452\u001b[0m model \u001b[39m=\u001b[39m FlaxLLaMAForCausalLM(config, dummy_input_shape, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    453\u001b[0m hf_model \u001b[39m=\u001b[39m transformers\u001b[39m.\u001b[39mAutoModelForCausalLM\u001b[39m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    454\u001b[0m     model_args\u001b[39m.\u001b[39mmodel_name_or_path,\n\u001b[1;32m    455\u001b[0m     torch_dtype\u001b[39m=\u001b[39mtorch_dtype\n\u001b[1;32m    456\u001b[0m )\n\u001b[1;32m    457\u001b[0m loaded_params \u001b[39m=\u001b[39m hf_to_jax_weight(hf_model)\n",
      "File \u001b[0;32m~/alpa/EasyLM/EasyLM/models/llama/llama_model.py:644\u001b[0m, in \u001b[0;36mFlaxLLaMAPreTrainedModel.__init__\u001b[0;34m(self, config, input_shape, seed, dtype, _do_init, **kwargs)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m    635\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    636\u001b[0m     config: LLaMAConfig,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    642\u001b[0m ):\n\u001b[1;32m    643\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule_class(config\u001b[39m=\u001b[39mconfig, dtype\u001b[39m=\u001b[39mdtype, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 644\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(config, module, input_shape\u001b[39m=\u001b[39;49minput_shape, seed\u001b[39m=\u001b[39;49mseed, dtype\u001b[39m=\u001b[39;49mdtype, _do_init\u001b[39m=\u001b[39;49m_do_init)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/modeling_flax_utils.py:209\u001b[0m, in \u001b[0;36mFlaxPreTrainedModel.__init__\u001b[0;34m(self, config, module, input_shape, seed, dtype, _do_init)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_initialized \u001b[39m=\u001b[39m _do_init\n\u001b[1;32m    207\u001b[0m \u001b[39mif\u001b[39;00m _do_init:\n\u001b[1;32m    208\u001b[0m     \u001b[39m# randomly initialized parameters\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     random_params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_weights(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey, input_shape)\n\u001b[1;32m    210\u001b[0m     params_shape_tree \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39meval_shape(\u001b[39mlambda\u001b[39;00m params: params, random_params)\n\u001b[1;32m    211\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/alpa/EasyLM/EasyLM/models/llama/llama_model.py:651\u001b[0m, in \u001b[0;36mFlaxLLaMAPreTrainedModel.init_weights\u001b[0;34m(self, rng, input_shape, params)\u001b[0m\n\u001b[1;32m    649\u001b[0m attention_mask \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mones_like(input_ids)\n\u001b[1;32m    650\u001b[0m position_ids \u001b[39m=\u001b[39m jnp\u001b[39m.\u001b[39mbroadcast_to(jnp\u001b[39m.\u001b[39marange(jnp\u001b[39m.\u001b[39matleast_2d(input_ids)\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]), input_shape)\n\u001b[0;32m--> 651\u001b[0m params_rng, dropout_rng \u001b[39m=\u001b[39m jax\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49msplit(rng)\n\u001b[1;32m    652\u001b[0m rngs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m: params_rng, \u001b[39m\"\u001b[39m\u001b[39mdropout\u001b[39m\u001b[39m\"\u001b[39m: dropout_rng}\n\u001b[1;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39madd_cross_attention:\n",
      "File \u001b[0;32m~/alpa/alpa/third_party/jax/jax/_src/random.py:213\u001b[0m, in \u001b[0;36msplit\u001b[0;34m(key, num)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Splits a PRNG key into `num` new keys by adding a leading axis.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \n\u001b[1;32m    204\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39m  An array-like object of `num` new PRNG keys.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    212\u001b[0m key, wrapped \u001b[39m=\u001b[39m _check_prng_key(key)\n\u001b[0;32m--> 213\u001b[0m \u001b[39mreturn\u001b[39;00m _return_prng_keys(wrapped, _split(key, num))\n",
      "File \u001b[0;32m~/alpa/alpa/third_party/jax/jax/_src/random.py:199\u001b[0m, in \u001b[0;36m_split\u001b[0;34m(key, num)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39mndim:\n\u001b[1;32m    197\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39msplit accepts a single key, but was given a key array of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    198\u001b[0m                   \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshape \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m != (). Use jax.vmap for batching.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 199\u001b[0m \u001b[39mreturn\u001b[39;00m prng\u001b[39m.\u001b[39;49mrandom_split(key, count\u001b[39m=\u001b[39;49mnum)\n",
      "File \u001b[0;32m~/alpa/alpa/third_party/jax/jax/_src/prng.py:624\u001b[0m, in \u001b[0;36mrandom_split\u001b[0;34m(keys, count)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrandom_split\u001b[39m(keys, count):\n\u001b[0;32m--> 624\u001b[0m   \u001b[39mreturn\u001b[39;00m random_split_p\u001b[39m.\u001b[39;49mbind(keys, count\u001b[39m=\u001b[39;49mcount)\n",
      "File \u001b[0;32m~/alpa/alpa/third_party/jax/jax/core.py:328\u001b[0m, in \u001b[0;36mPrimitive.bind\u001b[0;34m(self, *args, **params)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams):\n\u001b[1;32m    326\u001b[0m   \u001b[39massert\u001b[39;00m (\u001b[39mnot\u001b[39;00m config\u001b[39m.\u001b[39mjax_enable_checks \u001b[39mor\u001b[39;00m\n\u001b[1;32m    327\u001b[0m           \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(arg, Tracer) \u001b[39mor\u001b[39;00m valid_jaxtype(arg) \u001b[39mfor\u001b[39;00m arg \u001b[39min\u001b[39;00m args)), args\n\u001b[0;32m--> 328\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbind_with_trace(find_top_trace(args), args, params)\n",
      "File \u001b[0;32m~/alpa/alpa/third_party/jax/jax/core.py:331\u001b[0m, in \u001b[0;36mPrimitive.bind_with_trace\u001b[0;34m(self, trace, args, params)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbind_with_trace\u001b[39m(\u001b[39mself\u001b[39m, trace, args, params):\n\u001b[0;32m--> 331\u001b[0m   out \u001b[39m=\u001b[39m trace\u001b[39m.\u001b[39;49mprocess_primitive(\u001b[39mself\u001b[39;49m, \u001b[39mmap\u001b[39;49m(trace\u001b[39m.\u001b[39;49mfull_raise, args), params)\n\u001b[1;32m    332\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mmap\u001b[39m(full_lower, out) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmultiple_results \u001b[39melse\u001b[39;00m full_lower(out)\n",
      "File \u001b[0;32m~/alpa/alpa/third_party/jax/jax/core.py:698\u001b[0m, in \u001b[0;36mEvalTrace.process_primitive\u001b[0;34m(self, primitive, tracers, params)\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_primitive\u001b[39m(\u001b[39mself\u001b[39m, primitive, tracers, params):\n\u001b[0;32m--> 698\u001b[0m   \u001b[39mreturn\u001b[39;00m primitive\u001b[39m.\u001b[39;49mimpl(\u001b[39m*\u001b[39;49mtracers, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n",
      "File \u001b[0;32m~/alpa/alpa/third_party/jax/jax/_src/prng.py:636\u001b[0m, in \u001b[0;36mrandom_split_impl\u001b[0;34m(keys, count)\u001b[0m\n\u001b[1;32m    634\u001b[0m \u001b[39m@random_split_p\u001b[39m\u001b[39m.\u001b[39mdef_impl\n\u001b[1;32m    635\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrandom_split_impl\u001b[39m(keys, \u001b[39m*\u001b[39m, count):\n\u001b[0;32m--> 636\u001b[0m   base_arr \u001b[39m=\u001b[39m random_split_impl_base(\n\u001b[1;32m    637\u001b[0m       keys\u001b[39m.\u001b[39;49mimpl, keys\u001b[39m.\u001b[39;49munsafe_raw_array(), keys\u001b[39m.\u001b[39;49mndim, count\u001b[39m=\u001b[39;49mcount)\n\u001b[1;32m    638\u001b[0m   \u001b[39mreturn\u001b[39;00m PRNGKeyArray(keys\u001b[39m.\u001b[39mimpl, base_arr)\n",
      "File \u001b[0;32m~/alpa/alpa/third_party/jax/jax/_src/prng.py:642\u001b[0m, in \u001b[0;36mrandom_split_impl_base\u001b[0;34m(impl, base_arr, keys_ndim, count)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrandom_split_impl_base\u001b[39m(impl, base_arr, keys_ndim, \u001b[39m*\u001b[39m, count):\n\u001b[1;32m    641\u001b[0m   split \u001b[39m=\u001b[39m iterated_vmap_unary(keys_ndim, \u001b[39mlambda\u001b[39;00m k: impl\u001b[39m.\u001b[39msplit(k, count))\n\u001b[0;32m--> 642\u001b[0m   \u001b[39mreturn\u001b[39;00m split(base_arr)\n",
      "File \u001b[0;32m~/alpa/alpa/third_party/jax/jax/_src/prng.py:641\u001b[0m, in \u001b[0;36mrandom_split_impl_base.<locals>.<lambda>\u001b[0;34m(k)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrandom_split_impl_base\u001b[39m(impl, base_arr, keys_ndim, \u001b[39m*\u001b[39m, count):\n\u001b[0;32m--> 641\u001b[0m   split \u001b[39m=\u001b[39m iterated_vmap_unary(keys_ndim, \u001b[39mlambda\u001b[39;00m k: impl\u001b[39m.\u001b[39;49msplit(k, count))\n\u001b[1;32m    642\u001b[0m   \u001b[39mreturn\u001b[39;00m split(base_arr)\n",
      "File \u001b[0;32m~/alpa/alpa/third_party/jax/jax/_src/prng.py:1033\u001b[0m, in \u001b[0;36mthreefry_split\u001b[0;34m(key, num)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mthreefry_split\u001b[39m(key: jnp\u001b[39m.\u001b[39mndarray, num: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m jnp\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m-> 1033\u001b[0m   \u001b[39mreturn\u001b[39;00m _threefry_split(key, \u001b[39mint\u001b[39;49m(num))\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "File \u001b[0;32m~/alpa/alpa/third_party/jax/jax/_src/dispatch.py:878\u001b[0m, in \u001b[0;36m_execute_compiled\u001b[0;34m(name, compiled, input_handler, output_buffer_counts, result_handler, has_unordered_effects, ordered_effects, kept_var_idx, has_host_callbacks, *args)\u001b[0m\n\u001b[1;32m    876\u001b[0m     runtime_token \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 878\u001b[0m   out_flat \u001b[39m=\u001b[39m compiled\u001b[39m.\u001b[39;49mexecute(in_flat)\n\u001b[1;32m    879\u001b[0m check_special(name, out_flat)\n\u001b[1;32m    880\u001b[0m out_bufs \u001b[39m=\u001b[39m unflatten(out_flat, output_buffer_counts)\n",
      "\u001b[0;31mXlaRuntimeError\u001b[0m: INTERNAL: CustomCall failed: jaxlib/cuda/cuda_prng_kernels.cc:32: operation cudaGetLastError() failed: out of memory"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pre-training/Fine-tuning the library models for causal language modeling (GPT, GPT-2, CTRL, ...) on a text file or a dataset.\n",
    "\n",
    "Here is the full list of checkpoints on the hub that can be fine-tuned by this script:\n",
    "https://huggingface.co/models?filter=text-generation\n",
    "\"\"\"\n",
    "# You can also adapt this script on your own causal language modeling task. Pointers for this are left as comments.\n",
    "\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import asdict, dataclass, field\n",
    "from enum import Enum\n",
    "import functools\n",
    "from pathlib import Path\n",
    "from typing import Callable, Optional\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import alpa\n",
    "from alpa.model.model_util import DynamicScale, TrainState\n",
    "from alpa import ManualShardingOption\n",
    "import jax\n",
    "from jax.experimental.pjit import PartitionSpec\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import transformers\n",
    "from transformers.utils import get_full_repo_name, send_example_telemetry\n",
    "import tensorflow as tf\n",
    "from flax import traverse_util\n",
    "from optax import tree_map_params\n",
    "from huggingface_hub import Repository\n",
    "from transformers import (\n",
    "    FLAX_MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    HfArgumentParser,\n",
    "    is_tensorboard_available,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "import torch\n",
    "\n",
    "# alpa.init(cluster=\"ray\")\n",
    "\n",
    "# tf.config.experimental.set_visible_devices([], 'GPU')\n",
    "\n",
    "from EasyLM.models.llama.llama_model import (\n",
    "    LLaMAConfig, FlaxLLaMAForCausalLM\n",
    ")\n",
    "\n",
    "from hf_datasets import make_supervised_data_module\n",
    "from hf_jax_conversion import hf_to_jax_weight\n",
    "from monkey_patch import do_monkey_patch\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "MODEL_CONFIG_CLASSES = list(FLAX_MODEL_FOR_CAUSAL_LM_MAPPING.keys())\n",
    "MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n",
    "IGNORE_TOKEN_ID = -100\n",
    "print(jax.devices())\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments:\n",
    "    \"\"\"A subset of Huggingface's training arguments\"\"\"\n",
    "    output_dir: str = field(\n",
    "        default=\"./output\",\n",
    "        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n",
    "    )\n",
    "    overwrite_output_dir: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Overwrite the content of the output directory. \"\n",
    "                \"Use this to continue training if output_dir points to a checkpoint directory.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    do_train: bool = field(default=True, metadata={\"help\": \"Whether to run training.\"})\n",
    "    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the dev set.\"})\n",
    "    per_device_train_batch_size: int = field(\n",
    "        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for training.\"}\n",
    "    )\n",
    "    per_device_eval_batch_size: int = field(\n",
    "        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for evaluation.\"}\n",
    "    )\n",
    "    num_micro_batches: int = field(default=1, metadata={\"help\": \"The number of micro batches for gradient accumulation.\"})\n",
    "    operator_parallel: int = field(default=1, metadata={\"help\": \"The degree of operator model parallelism.\"})\n",
    "    pipeline_parallel: int = field(default=1, metadata={\"help\": \"The degree of pipeline model parallelism.\"})\n",
    "    use_remat: bool = field(default=True, metadata={\"help\": \"Whether or not to use gradient rematerilization/gradient checkpointing.\"})\n",
    "    learning_rate: float = field(default=5e-5, metadata={\"help\": \"The initial learning rate for AdamW.\"})\n",
    "    weight_decay: float = field(default=0.0, metadata={\"help\": \"Weight decay for AdamW if we apply some.\"})\n",
    "    adam_beta1: float = field(default=0.9, metadata={\"help\": \"Beta1 for AdamW optimizer\"})\n",
    "    adam_beta2: float = field(default=0.999, metadata={\"help\": \"Beta2 for AdamW optimizer\"})\n",
    "    adam_epsilon: float = field(default=1e-8, metadata={\"help\": \"Epsilon for AdamW optimizer.\"})\n",
    "    adafactor: bool = field(default=False, metadata={\"help\": \"Whether or not to replace AdamW by Adafactor.\"})\n",
    "    num_train_epochs: float = field(default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"})\n",
    "    warmup_ratio: float = field(default=0.0, metadata={\"help\": \"Linear warmup over a ratio of overall steps.\"})\n",
    "    logging_steps: int = field(default=500, metadata={\"help\": \"Log every X updates steps.\"})\n",
    "    save_steps: int = field(default=500, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n",
    "    eval_steps: int = field(default=None, metadata={\"help\": \"Run an evaluation every X steps.\"})\n",
    "    seed: int = field(default=42, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"})\n",
    "    push_to_hub: bool = field(\n",
    "        default=False, metadata={\"help\": \"Whether or not to upload the trained model to the model hub after training.\"}\n",
    "    )\n",
    "    hub_model_id: str = field(\n",
    "        default=None, metadata={\"help\": \"The name of the repository to keep in sync with the local `output_dir`.\"}\n",
    "    )\n",
    "    hub_token: str = field(default=None, metadata={\"help\": \"The token to use to push to the Model Hub.\"})\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.output_dir is not None:\n",
    "            self.output_dir = os.path.expanduser(self.output_dir)\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"\n",
    "        Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates\n",
    "        the token values by removing their value.\n",
    "        \"\"\"\n",
    "        d = asdict(self)\n",
    "        for k, v in d.items():\n",
    "            if isinstance(v, Enum):\n",
    "                d[k] = v.value\n",
    "            if isinstance(v, list) and len(v) > 0 and isinstance(v[0], Enum):\n",
    "                d[k] = [x.value for x in v]\n",
    "            if k.endswith(\"_token\"):\n",
    "                d[k] = f\"<{k.upper()}>\"\n",
    "        return d\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n",
    "    \"\"\"\n",
    "\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=\"/data/yonghao.zhuang/vicuna-7b-v1.2-b128l2\",\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"The model checkpoint for weights initialization.Don't set if you want to train a model from scratch.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    model_type: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n",
    "    )\n",
    "    config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n",
    "    )\n",
    "    tokenizer_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n",
    "    )\n",
    "    cache_dir: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n",
    "    )\n",
    "    use_fast_tokenizer: bool = field(\n",
    "        default=True,\n",
    "        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n",
    "    )\n",
    "    dtype: Optional[str] = field(\n",
    "        default=\"float16\",\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Floating-point format in which the model weights should be initialized and trained. Choose one of\"\n",
    "                \" `[float32, float16, bfloat16]`.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    use_auth_token: bool = field(\n",
    "        default=False,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Will use the token generated when running `transformers-cli login` (necessary to use this script \"\n",
    "                \"with private models).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataTrainingArguments:\n",
    "    \"\"\"\n",
    "    Arguments pertaining to what data we are going to input our model for training and eval.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name: Optional[str] = field(\n",
    "        default=\"/home/yonghao.zhuang/alpa/files/sharegpt/sharegpt_20230422_clean_lang_split_identity.json\", metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    dataset_config_name: Optional[str] = field(\n",
    "        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n",
    "    )\n",
    "    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n",
    "    validation_file: Optional[str] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n",
    "    )\n",
    "    max_train_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    max_eval_samples: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
    "                \"value if set.\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    validation_split_percentage: Optional[int] = field(\n",
    "        default=5,\n",
    "        metadata={\n",
    "            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n",
    "        },\n",
    "    )\n",
    "    block_size: Optional[int] = field(\n",
    "        default=1024,\n",
    "        metadata={\n",
    "            \"help\": (\n",
    "                \"Optional input sequence length after tokenization. \"\n",
    "                \"The training dataset will be truncated in block of this size for training. \"\n",
    "                \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n",
    "            )\n",
    "        },\n",
    "    )\n",
    "    overwrite_cache: bool = field(\n",
    "        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n",
    "    )\n",
    "    preprocessing_num_workers: Optional[int] = field(\n",
    "        default=None,\n",
    "        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n",
    "    )\n",
    "    keep_linebreaks: bool = field(\n",
    "        default=True, metadata={\"help\": \"Whether to keep line breaks when using TXT files or not.\"}\n",
    "    )\n",
    "\n",
    "    def __post_init__(self):\n",
    "        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n",
    "            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n",
    "        else:\n",
    "            if self.train_file is not None:\n",
    "                extension = self.train_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, a json or a txt file.\"\n",
    "            if self.validation_file is not None:\n",
    "                extension = self.validation_file.split(\".\")[-1]\n",
    "                assert extension in [\"csv\", \"json\", \"txt\"], \"`validation_file` should be a csv, a json or a txt file.\"\n",
    "\n",
    "\n",
    "def data_loader(rng: jax.random.PRNGKey, dataset: Dataset, batch_size: int,\n",
    "                min_batch_size: int, shuffle: bool = False):\n",
    "    \"\"\"\n",
    "    Returns batches of size `batch_size` from truncated `dataset`, sharded over all local devices.\n",
    "    Shuffle batches if `shuffle` is `True`.\n",
    "    \"\"\"\n",
    "    if len(dataset) < batch_size:\n",
    "        assert len(dataset) >= min_batch_size\n",
    "        batch_size = len(dataset) // min_batch_size * min_batch_size\n",
    "\n",
    "    data_collator = transformers.DefaultDataCollator(\"np\")\n",
    "    tf_dataset = dataset.to_tf_dataset(batch_size=batch_size,\n",
    "                                       columns=dataset.column_names,\n",
    "                                       collate_fn=data_collator,\n",
    "                                       shuffle=shuffle,\n",
    "                                       drop_remainder=True)\n",
    "\n",
    "    for batch in tf_dataset:\n",
    "        batch = {k: v._numpy() for k, v in batch.items()}\n",
    "        yield batch\n",
    "\n",
    "\n",
    "def write_train_metric(summary_writer, train_metrics, train_time, step):\n",
    "    summary_writer.scalar(\"train_time\", train_time, step)\n",
    "\n",
    "    train_metrics = alpa.util.get_metrics(train_metrics)\n",
    "    for key, vals in train_metrics.items():\n",
    "        tag = f\"train_{key}\"\n",
    "        for i, val in enumerate(vals):\n",
    "            summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n",
    "\n",
    "\n",
    "def write_eval_metric(summary_writer, eval_metrics, step):\n",
    "    for metric_name, value in eval_metrics.items():\n",
    "        summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n",
    "\n",
    "\n",
    "def create_learning_rate_fn(\n",
    "    train_ds_size: int, train_batch_size: int, num_train_epochs: int, warmup_ratio: float, learning_rate: float\n",
    ") -> Callable[[int], jnp.array]:\n",
    "    \"\"\"Returns a linear warmup, linear_decay learning rate function.\"\"\"\n",
    "    steps_per_epoch = train_ds_size // train_batch_size\n",
    "    num_train_steps = steps_per_epoch * num_train_epochs\n",
    "    num_warmup_steps = int(num_train_steps * warmup_ratio)\n",
    "    warmup_fn = optax.linear_schedule(init_value=0.0, end_value=learning_rate, transition_steps=num_warmup_steps)\n",
    "    decay_fn = optax.cosine_decay_schedule(\n",
    "        init_value=learning_rate, decay_steps=num_train_steps - num_warmup_steps\n",
    "    )\n",
    "    schedule_fn = optax.join_schedules(schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])\n",
    "    return schedule_fn\n",
    "\n",
    "\n",
    "def llama_manual_sharding(num_layers, state: TrainState):\n",
    "    # TODO: when rebased to jax 0.4.6, use the tree_map_with_path\n",
    "    param_partition = {\n",
    "        'transformer': {\n",
    "            'wte': {'embedding': PartitionSpec(\"mp\", None)},\n",
    "            'ln_f': {'kernel': PartitionSpec(None)},\n",
    "            'h': {\n",
    "                '%d' % (layer): {\n",
    "                    'attention': {\n",
    "                        'wq': {'kernel': PartitionSpec(None, \"mp\")},\n",
    "                        'wk': {'kernel': PartitionSpec(None, \"mp\")},\n",
    "                        'wv': {'kernel': PartitionSpec(None, \"mp\")},\n",
    "                        'wo': {'kernel': PartitionSpec(\"mp\", None)},\n",
    "                    },\n",
    "                    'feed_forward': {\n",
    "                        'w1': {'kernel': PartitionSpec(None, \"mp\")},\n",
    "                        'w2': {'kernel': PartitionSpec(\"mp\", None)},\n",
    "                        'w3': {'kernel': PartitionSpec(None, \"mp\")},\n",
    "                    },\n",
    "                    'attention_norm': {'kernel': PartitionSpec(None)},\n",
    "                    'ffn_norm': {'kernel': PartitionSpec(None)},\n",
    "                }\n",
    "            for layer in range(num_layers)},\n",
    "        },\n",
    "        'lm_head': {'kernel': PartitionSpec(None, \"mp\")},\n",
    "    }\n",
    "    replicate = lambda x : jax.tree_util.tree_map(lambda _: PartitionSpec(None), x)\n",
    "    opt_state = tree_map_params(state.tx, lambda _, spec: spec, state.opt_state,\n",
    "                                param_partition, transform_non_params=lambda _: PartitionSpec(None))\n",
    "    manual_partition = TrainState(\n",
    "        step=PartitionSpec(None),\n",
    "        params=param_partition,\n",
    "        master_copy=param_partition if state.master_copy else None,\n",
    "        dynamic_scale=replicate(state.dynamic_scale),\n",
    "        tx=state.tx,\n",
    "        apply_fn=state.apply_fn,\n",
    "        opt_state=opt_state)\n",
    "    return manual_partition\n",
    "\n",
    "\n",
    "# TODO: smoothing factor\n",
    "def loss_fn(logits, labels, ignore_indices):\n",
    "    # Shift logits\n",
    "    shift_logits = logits[..., :-1, :]\n",
    "    shift_labels = labels[..., 1:]\n",
    "    # Handle the ignore index: compute the valid first\n",
    "    valid = jnp.full(shift_labels.shape, True)\n",
    "    for ignore_index in ignore_indices:\n",
    "        new_valid = jnp.not_equal(shift_labels, ignore_index)\n",
    "        valid = jnp.logical_and(valid, new_valid)\n",
    "    valid = jnp.asarray(valid, dtype=jnp.float32)\n",
    "    valid_len = jnp.maximum(jnp.sum(valid, axis=-1), 1e-10)\n",
    "    # OneHot and mask the ignore index. For ignore_index(-100), the whole line\n",
    "    # in the output would be 0.\n",
    "    one_hot_labels = jax.nn.one_hot(shift_labels, shift_logits.shape[-1])\n",
    "    # Compute the softmax loss\n",
    "    log_p = jax.nn.log_softmax(shift_logits, axis=-1)\n",
    "    # (bs, seq_len, vocab) -> (bs, seq_len)\n",
    "    cross_entropy = jnp.sum(one_hot_labels * log_p, axis=-1)\n",
    "    loss = -jnp.mean(jnp.sum(cross_entropy * valid, axis=-1) / valid_len)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# See all possible arguments in src/transformers/training_args.py\n",
    "# or by passing the --help flag to this script.\n",
    "# We now keep distinct sets of args, for a cleaner separation of concerns.\n",
    "\n",
    "model_args, data_args, training_args = ModelArguments(), DataTrainingArguments(), TrainingArguments()\n",
    "\n",
    "if (\n",
    "    os.path.exists(training_args.output_dir)\n",
    "    and os.listdir(training_args.output_dir)\n",
    "    and training_args.do_train\n",
    "    and not training_args.overwrite_output_dir\n",
    "):\n",
    "    raise ValueError(\n",
    "        f\"Output directory ({training_args.output_dir}) already exists and is not empty.\"\n",
    "        \"Use --overwrite_output_dir to overcome.\"\n",
    "    )\n",
    "\n",
    "# Make one log on every process with the configuration for debugging.\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "# Setup logging, we only want one process per machine to log things on the screen.\n",
    "logger.setLevel(logging.INFO)\n",
    "datasets.utils.logging.set_verbosity_warning()\n",
    "transformers.utils.logging.set_verbosity_info()\n",
    "\n",
    "# Set the verbosity to info of the Transformers logger (on main process only):\n",
    "logger.info(f\"Training/evaluation parameters {training_args}\")\n",
    "\n",
    "# Set seed before initializing model.\n",
    "set_seed(training_args.seed)\n",
    "\n",
    "# See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
    "# https://huggingface.co/docs/datasets/loading_datasets.html.\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "config = LLaMAConfig.load_config('7b')\n",
    "if model_args.dtype == \"float16\":\n",
    "    dtype = jnp.float16\n",
    "    torch_dtype = torch.float16\n",
    "elif model_args.dtype == \"float32\":\n",
    "    dtype = jnp.float32\n",
    "    torch_dtype = torch.float32\n",
    "elif model_args.dtype == \"bfloat16\":\n",
    "    dtype = jnp.bfloat16\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    raise ValueError(f\"{model_args.dtype} unsupported\")\n",
    "# TODO: set the correct remat policy.\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    model_max_length=config.max_sequence_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    "    dtype=model_args.dtype\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "config.update(dict(\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "))\n",
    "\n",
    "# TODO(yonghao): don't init weight when loaded somewhere\n",
    "dummy_input_shape = (4, config.max_sequence_length)\n",
    "# Monkey patch the model's init to init_dummy\n",
    "do_monkey_patch()\n",
    "model = FlaxLLaMAForCausalLM(config, dummy_input_shape, dtype=dtype)\n",
    "hf_model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    torch_dtype=torch_dtype\n",
    ")\n",
    "loaded_params = hf_to_jax_weight(hf_model)\n",
    "\n",
    "#  Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n",
    "# or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
    "# (the dataset will be downloaded automatically from the datasets Hub).\n",
    "#\n",
    "# For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n",
    "# 'text' is found. You can easily tweak this behavior (see below).\n",
    "#\n",
    "# In distributed training, the load_dataset function guarantees that only one local process can concurrently\n",
    "# download the dataset.\n",
    "data_module = make_supervised_data_module(tokenizer, data_args.dataset_name, IGNORE_TOKEN_ID)\n",
    "\n",
    "\n",
    "if data_args.block_size is None:\n",
    "    block_size = tokenizer.model_max_length\n",
    "    if block_size > config.max_position_embeddings:\n",
    "        logger.warning(\n",
    "            f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n",
    "            \"Picking 1024 instead. You can change that default value by passing --block_size xxx.\"\n",
    "        )\n",
    "        block_size = 1024\n",
    "else:\n",
    "    if data_args.block_size > tokenizer.model_max_length:\n",
    "        logger.warning(\n",
    "            f\"The block_size passed ({data_args.block_size}) is larger than the maximum length for the model\"\n",
    "            f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n",
    "        )\n",
    "    block_size = min(data_args.block_size, tokenizer.model_max_length)\n",
    "\n",
    "# Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a remainder\n",
    "# for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value might be slower\n",
    "# to preprocess.\n",
    "#\n",
    "# To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "# https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "\n",
    "logger.info(\"***** Build dataset *****\")\n",
    "\n",
    "if training_args.do_train:\n",
    "    if \"train_dataset\" not in data_module:\n",
    "        raise ValueError(\"--do_train requires a train dataset\")\n",
    "    train_dataset = data_module[\"train_dataset\"]\n",
    "    if data_args.max_train_samples is not None:\n",
    "        max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n",
    "        train_dataset = train_dataset.select(range(max_train_samples))\n",
    "\n",
    "if training_args.do_eval:\n",
    "    if \"eval_dataset\" not in data_module:\n",
    "        raise ValueError(\"--do_eval requires a validation dataset\")\n",
    "    eval_dataset = data_module[\"eval_dataset\"]\n",
    "    if data_args.max_eval_samples is not None:\n",
    "        max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n",
    "        eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
    "\n",
    "# Adjust batch size and num_micro_batches for small datasets\n",
    "# num_devices = alpa.get_global_num_devices()\n",
    "num_devices = 8\n",
    "train_min_batch_size = (num_devices // training_args.operator_parallel //\n",
    "                        training_args.pipeline_parallel * training_args.num_micro_batches)\n",
    "eval_num_micro_batches = training_args.num_micro_batches\n",
    "eval_min_batch_size = (num_devices // training_args.operator_parallel //\n",
    "                        training_args.pipeline_parallel * eval_num_micro_batches)\n",
    "while training_args.do_eval and (len(eval_dataset) < eval_min_batch_size):\n",
    "    eval_num_micro_batches //= 2\n",
    "    eval_min_batch_size = (num_devices // training_args.operator_parallel //\n",
    "                            training_args.pipeline_parallel * eval_num_micro_batches)\n",
    "\n",
    "# Initialize our training\n",
    "rng = jax.random.PRNGKey(training_args.seed)\n",
    "rng, dropout_rng = jax.random.split(rng)\n",
    "\n",
    "# Store some constant\n",
    "num_epochs = int(training_args.num_train_epochs)\n",
    "train_batch_size = int(training_args.per_device_train_batch_size) * num_devices\n",
    "eval_batch_size = int(training_args.per_device_eval_batch_size) * num_devices\n",
    "steps_per_epoch = len(train_dataset) // train_batch_size\n",
    "total_train_steps = steps_per_epoch * num_epochs\n",
    "\n",
    "# Create learning rate schedule\n",
    "cosine_decay_lr_schedule_fn = create_learning_rate_fn(\n",
    "    len(train_dataset),\n",
    "    train_batch_size,\n",
    "    training_args.num_train_epochs,\n",
    "    training_args.warmup_ratio,\n",
    "    training_args.learning_rate,\n",
    ")\n",
    "\n",
    "# We use Optax's \"masking\" functionality to not apply weight decay\n",
    "# to bias and LayerNorm scale parameters. decay_mask_fn returns a\n",
    "# mask boolean with the same structure as the parameters.\n",
    "# The mask is True for parameters that should be decayed.\n",
    "# Note that this mask is specifically adapted for FlaxGPT2.\n",
    "# For other models, one should correct the layer norm parameter naming\n",
    "# accordingly.\n",
    "def decay_mask_fn(params):\n",
    "    flat_params = traverse_util.flatten_dict(params)\n",
    "    flat_mask = {\n",
    "        path: (path[-1] != \"bias\" and path[-2:] not in [(\"ln_1\", \"scale\"), (\"ln_2\", \"scale\"), (\"ln_f\", \"scale\")])\n",
    "        for path in flat_params\n",
    "    }\n",
    "    return traverse_util.unflatten_dict(flat_mask)\n",
    "\n",
    "# create adam optimizer\n",
    "if training_args.adafactor:\n",
    "    # We use the default parameters here to initialize adafactor,\n",
    "    # For more details about the parameters please check https://github.com/deepmind/optax/blob/ed02befef9bf81cbbf236be3d2b0e032e9ed4a40/optax/_src/alias.py#L74\n",
    "    optimizer = optax.adafactor(\n",
    "        learning_rate=cosine_decay_lr_schedule_fn,\n",
    "    )\n",
    "else:\n",
    "    # A tmp hack for llama finetune. Remove it either:\n",
    "    # 1) rebase to jax 0.4 and use tree_util's mask with path for partition spec;\n",
    "    # 2) optax fixes the issue of symbolic exec with decay mask fn.\n",
    "    if training_args.weight_decay == 0.0:\n",
    "        decay_mask_fn = None\n",
    "    optimizer = optax.chain(\n",
    "        optax.clip_by_global_norm(1.0),\n",
    "        optax.adamw(\n",
    "            learning_rate=cosine_decay_lr_schedule_fn,\n",
    "            b1=training_args.adam_beta1,\n",
    "            b2=training_args.adam_beta2,\n",
    "            eps=training_args.adam_epsilon,\n",
    "            weight_decay=training_args.weight_decay,\n",
    "            mask=decay_mask_fn)\n",
    "    )\n",
    "\n",
    "# Setup train state\n",
    "if model_args.dtype == \"float16\":\n",
    "    use_master_copy = True\n",
    "    dynamic_scale = DynamicScale()\n",
    "    # Fix a bug in huggingface's implementation (https://github.com/huggingface/transformers/pull/18462)\n",
    "    alpa.global_config.flax_always_use_fp16_embedding = True\n",
    "else:\n",
    "    use_master_copy = dynamic_scale = None\n",
    "# state = TrainState.create(apply_fn=model.__call__, params=loaded_params, tx=optimizer,\n",
    "#                             dynamic_scale=dynamic_scale, use_master_copy=use_master_copy)\n",
    "\n",
    "# # Manual partition spec\n",
    "# state_manual_sharding = llama_manual_sharding(config.num_hidden_layers, state)\n",
    "# ms_option = ManualShardingOption(\n",
    "#     (\"dp\", \"mp\"), in_axis_resources=(state_manual_sharding, PartitionSpec(\"dp\", None)))\n",
    "ignore_ids = (IGNORE_TOKEN_ID, tokenizer.pad_token_id)\n",
    "\n",
    "train_time = 0\n",
    "train_metrics = []\n",
    "\n",
    "step_ct = 0\n",
    "last_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng, input_rng = jax.random.split(rng)\n",
    "train_loader = data_loader(input_rng, train_dataset, 1,\n",
    "                                1, shuffle=False)\n",
    "steps_per_epoch = len(train_dataset) // train_batch_size\n",
    "\n",
    "batch = next(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids = batch[\"attention_mask\"].cumsum(-1) - 1\n",
    "# position_ids = np.select(batch[\"attention_mask\"] == 0, position_ids,\n",
    "#                          np.ones_like(position_ids))\n",
    "batch[\"position_ids\"] = position_ids\n",
    "\n",
    "torch_batch = {}\n",
    "for k in batch:\n",
    "    torch_batch[k] = torch.Tensor(batch[k]).to(torch.device(\"cuda:1\")).long()\n",
    "labels = batch.pop(\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "@partial(jax.jit, device=jax.devices(\"gpu\")[2])\n",
    "def compute_loss(params, batch):\n",
    "    # Currently we don't support non-deterministic training with remat,\n",
    "    # so train=False. This arg has no other impact.\n",
    "    outs = model(**batch, params=params, train=False, output_hidden_states=True)\n",
    "    logits = outs[0]\n",
    "    hidden_states = outs[1]\n",
    "    loss = loss_fn(logits, labels, ignore_ids)\n",
    "    return logits, loss, hidden_states\n",
    "logits, loss, hidden_states = compute_loss(loaded_params, batch)\n",
    "print(np.array(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model = hf_model.to(torch.device(\"cuda:1\"))\n",
    "with torch.no_grad():\n",
    "    hf_out = hf_model(**torch_batch, output_hidden_states=True)\n",
    "    hf_loss = hf_out.loss.detach().cpu().numpy()\n",
    "    hf_logits = hf_out.logits.detach().cpu().numpy()\n",
    "print(hf_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hf_loss, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_hid_0 = np.array(hidden_states[0])\n",
    "hf_hid_0 = hf_out.hidden_states[0].detach().cpu().numpy()\n",
    "print(np.allclose(jax_hid_0, hf_hid_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax_hid_1 = np.array(hidden_states[1])\n",
    "hf_hid_1 = hf_out.hidden_states[1].detach().cpu().numpy()\n",
    "print(np.allclose(jax_hid_1, hf_hid_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EasyLM.models.llama.llama_model import FlaxLLaMABlock\n",
    "\n",
    "block = FlaxLLaMABlock(config, dtype=dtype)\n",
    "block_param = loaded_params['transformer']['h']['0']\n",
    "@jax.jit\n",
    "def compute(block_param, hidden_state):\n",
    "    x = block.apply({\"params\": block_param}, hidden_state,\n",
    "                    batch[\"attention_mask\"],\n",
    "                    batch[\"position_ids\"])\n",
    "    return x\n",
    "manual_jax_hid_1 = compute(block_param, hidden_states[0])\n",
    "manual_jax_hid_1 = np.array(manual_jax_hid_1)\n",
    "print(np.allclose(jax_hid_1, manual_jax_hid_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_hid_attention_mask = hf_model.model._prepare_decoder_attention_mask(\n",
    "    torch_batch[\"attention_mask\"], torch_batch[\"input_ids\"].shape,\n",
    "    hf_out.hidden_states[0], 0)\n",
    "with torch.no_grad():\n",
    "    manual_hf_hid_1 = hf_model.model.layers[0](\n",
    "        hf_out.hidden_states[0], hf_hid_attention_mask,\n",
    "        torch_batch[\"position_ids\"])[0]\n",
    "manual_hf_hid_1 = manual_hf_hid_1.detach().cpu().numpy()\n",
    "print(np.allclose(hf_hid_1, manual_hf_hid_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EasyLM.models.llama.llama_model import FlaxLLaMAAttention, FlaxLLaMAMLP, RMSNorm\n",
    "flax_attn = FlaxLLaMAAttention(config, dtype=dtype, param_dtype=dtype)\n",
    "norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps, dtype=dtype, param_dtype=dtype)\n",
    "ffn = FlaxLLaMAMLP(config, dtype=dtype, param_dtype=dtype)\n",
    "\n",
    "attn_param = block_param[\"attention\"]\n",
    "ffn_param = block_param[\"feed_forward\"]\n",
    "attn_norm_param = block_param[\"attention_norm\"]\n",
    "ffn_norm_param = block_param[\"ffn_norm\"]\n",
    "\n",
    "@jax.jit\n",
    "def compute(block_param, hidden_state):\n",
    "    normed_hidden_state = norm.apply({\"params\": block_param[\"attention_norm\"]}, hidden_state)\n",
    "    attn_outputs = flax_attn.apply({\"params\": block_param[\"attention\"]}, normed_hidden_state,\n",
    "        attention_mask=batch[\"attention_mask\"],\n",
    "        position_ids=batch[\"position_ids\"],\n",
    "        fcm_mask=None\n",
    "    )\n",
    "    attn_output = attn_outputs[0]\n",
    "    hidden_state = hidden_state + attn_output\n",
    "\n",
    "    ffn_normed_hidden = norm.apply({\"params\": block_param[\"ffn_norm\"]}, hidden_state)\n",
    "    feed_forward_hidden_state = ffn.apply({\"params\": block_param[\"feed_forward\"]},\n",
    "        ffn_normed_hidden,\n",
    "        deterministic=True,\n",
    "    )\n",
    "    hidden_state = hidden_state + feed_forward_hidden_state\n",
    "    return hidden_state, (normed_hidden_state, attn_outputs[0], ffn_normed_hidden, feed_forward_hidden_state)\n",
    "split_manual_jax_hid_1, (normed_hidden, attn_out, ffn_normed, ffn_out) = compute(block_param, hidden_states[0])\n",
    "split_manual_jax_hid_1 = np.array(split_manual_jax_hid_1)\n",
    "print(np.allclose(jax_hid_1, split_manual_jax_hid_1))\n",
    "print(np.allclose(manual_jax_hid_1, split_manual_jax_hid_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.allclose(split_manual_jax_hid_1, hf_hid_1))\n",
    "# print(split_manual_jax_hid_1)\n",
    "# print(manual_jax_hid_1)\n",
    "# print(hf_hid_1)\n",
    "print(jax_hid_1.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_block = hf_model.model.layers[0]\n",
    "def hf_compute(hidden_states, attention_mask, position_ids):\n",
    "    with torch.no_grad():\n",
    "        residual = hidden_states\n",
    "        attn_normed = hf_block.input_layernorm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        attn_out, _, _ = hf_block.self_attn(\n",
    "            hidden_states=attn_normed,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "        hidden_states = residual + attn_out\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        ffn_normed = hf_block.post_attention_layernorm(hidden_states)\n",
    "        ffn_out = hf_block.mlp(ffn_normed)\n",
    "        hidden_states = residual + ffn_out\n",
    "    return hidden_states.detach().cpu().numpy(), (attn_normed, attn_out, ffn_normed, ffn_out)\n",
    "\n",
    "\n",
    "manual_hf_hid_1, (hf_attn_normed, hf_attn_out, hf_ffn_normed,\n",
    "                  hf_ffn_out) = hf_compute(hf_out.hidden_states[0],\n",
    "                                           hf_hid_attention_mask,\n",
    "                                           torch_batch[\"position_ids\"])\n",
    "print(np.allclose(hf_hid_1, manual_hf_hid_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.allclose(np.array(normed_hidden), hf_attn_normed.detach().cpu().numpy()))\n",
    "print(np.allclose(np.array(attn_out), hf_attn_out.detach().cpu().numpy(), atol=1e-7))\n",
    "print(np.allclose(np.array(ffn_normed), hf_ffn_normed.detach().cpu().numpy(), atol=1e-6))\n",
    "print(np.allclose(np.array(ffn_out), hf_ffn_out.detach().cpu().numpy(), atol=1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "dense_1 = nn.Dense(\n",
    "    config.intermediate_size,\n",
    "    dtype=dtype,\n",
    "    use_bias=False,\n",
    "    kernel_init=jax.nn.initializers.normal(config.initializer_range),\n",
    ")\n",
    "dense_2 = nn.Dense(\n",
    "    config.hidden_size,\n",
    "    dtype=dtype,\n",
    "    use_bias=False,\n",
    "    kernel_init=jax.nn.initializers.normal(config.initializer_range),\n",
    ")\n",
    "dense_3 = nn.Dense(\n",
    "    config.intermediate_size,\n",
    "    dtype=dtype,\n",
    "    use_bias=False,\n",
    "    kernel_init=jax.nn.initializers.normal(config.initializer_range),\n",
    ")\n",
    "mlp_block = hf_block.mlp\n",
    "\n",
    "@jax.jit\n",
    "def compute(x):\n",
    "    gate = dense_1.apply({\"params\": ffn_param[\"w1\"]}, x)\n",
    "    up = dense_3.apply({\"params\": ffn_param[\"w3\"]}, x)\n",
    "    x = dense_2.apply({\"params\": ffn_param[\"w2\"]}, nn.silu(gate) * up)\n",
    "    return x, gate, up\n",
    "def torch_compute(x):\n",
    "    with torch.no_grad():\n",
    "        gate = mlp_block.gate_proj(x)\n",
    "        up = mlp_block.up_proj(x)\n",
    "        x = mlp_block.down_proj(mlp_block.act_fn(gate) * up)\n",
    "    return x, gate, up\n",
    "\n",
    "hf_ffn_in = hf_ffn_normed.detach().cpu().numpy()\n",
    "print(np.allclose(ffn_param[\"w1\"][\"kernel\"], mlp_block.gate_proj.weight.detach().cpu().numpy().transpose()))\n",
    "print(np.allclose(ffn_param[\"w2\"][\"kernel\"], mlp_block.down_proj.weight.detach().cpu().numpy().transpose()))\n",
    "print(np.allclose(ffn_param[\"w3\"][\"kernel\"], mlp_block.up_proj.weight.detach().cpu().numpy().transpose()))\n",
    "\n",
    "\n",
    "ffn_out_hf_input, ffn_gate, ffn_up = [np.array(x) for x in compute(hf_ffn_in)]\n",
    "hf_ffn_out, hf_gate, hf_up = torch_compute(hf_ffn_normed)\n",
    "\n",
    "print(np.allclose(hf_ffn_out.detach().cpu().numpy(), mlp_block(hf_ffn_normed).detach().cpu().numpy()))\n",
    "print(np.allclose(hf_gate.detach().cpu().numpy(), ffn_gate, atol=1e-6))\n",
    "print(np.allclose(hf_up.detach().cpu().numpy(), ffn_up, atol=1e-6))\n",
    "print(np.allclose(ffn_out_hf_input, hf_block.mlp(hf_ffn_normed).detach().cpu().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hf_gate.detach().cpu().numpy())\n",
    "print(ffn_gate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_param[\"wq\"]\n",
    "attn_param[\"wk\"]\n",
    "attn_param[\"wv\"]\n",
    "attn_param[\"wo\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
