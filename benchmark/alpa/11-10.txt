Working on case: BenchmarkCase(batch_size=1024, model_config=GPTModelConfig(seq_len=1024, hidden_size=2048, num_layers=24, num_heads=32, vocab_size=51200), num_micro_batches=32, parallel_mode='load_solution', parallel_args=LoadSolutionParallelArgs(prefer_reduce_scatter=True, use_remat=True, num_auto_layers=6, forward_stage_layer_ids=[[0, 1, 2, 3, 4, 5]], submesh_physical_shapes=[[1, 4]], submesh_logical_shapes=[[4, 1]], submesh_autosharding_option_dicts=[{'force_batch_dim_to_mesh_dim': '0'}]))
 - Prepare input: 0.07 s
 - Create train state: 5.61 s
Working on case: BenchmarkCase(batch_size=1024, model_config=GPTModelConfig(seq_len=1024, hidden_size=2048, num_layers=24, num_heads=32, vocab_size=51200), num_micro_batches=32, parallel_mode='load_solution', parallel_args=LoadSolutionParallelArgs(prefer_reduce_scatter=True, use_remat=True, num_auto_layers=6, forward_stage_layer_ids=[[0, 1, 2, 3, 4, 5]], submesh_physical_shapes=[[1, 4]], submesh_logical_shapes=[[4, 1]], submesh_autosharding_option_dicts=[{'force_batch_dim_to_mesh_dim': 0}]))
 - Prepare input: 0.07 s
 - Create train state: 5.31 s
 - Compile (driver): 67.31 s
 - Compile (worker): 56.93 s
Iteration 0 ...
Iteration 1 ...
Iteration 2 ...
 - Benchmark: 158.08 s
Type: gpt  Model Config: GPTModelConfig(seq_len=1024, hidden_size=2048, num_layers=24, num_heads=32, vocab_size=51200)  #Microbatch: 32  #GPU: 4  Parallel Config: LoadSolutionParallelArgs(prefer_reduce_scatter=True, use_remat=True, num_auto_layers=6, forward_stage_layer_ids=[[0, 1, 2, 3, 4, 5]], submesh_physical_shapes=[[1, 4]], submesh_logical_shapes=[[4, 1]], submesh_autosharding_option_dicts=[{'force_batch_dim_to_mesh_dim': 0}])  Mean Time (s): 49.219  Std Time (s): 0.003  #Params (Billion): 1.313B  TFLOPs: 59.11  Peak Mem (GB): 13.137  Metadata: {'compilation_times': 'None', 'compute_cost_file_name': 'None', 'forward_stage_layer_ids': 'None', 'submesh_shapes': 'None', 'logical_mesh_shapes': 'None', 'autosharding_option_dicts': 'None'}  
running spends 4min 57s
Working on case: BenchmarkCase(batch_size=1024, model_config=GPTModelConfig(seq_len=1024, hidden_size=1536, num_layers=24, num_heads=16, vocab_size=51200), num_micro_batches=64, parallel_mode='load_solution', parallel_args=LoadSolutionParallelArgs(prefer_reduce_scatter=True, use_remat=True, num_auto_layers=6, forward_stage_layer_ids=[[0, 1, 2], [3, 4, 5]], submesh_physical_shapes=[[1, 1], [1, 1]], submesh_logical_shapes=[[1, 1], [1, 1]], submesh_autosharding_option_dicts=[{'force_batch_dim_to_mesh_dim': 0}, {}]))
 - Prepare input: 0.07 s
 - Create train state: 5.30 s
 - Compile (driver): 50.00 s
 - Compile (worker): 66.57 s
Iteration 0 ...
Iteration 1 ...
Iteration 2 ...
 - Benchmark: 180.44 s
Type: gpt  Model Config: GPTModelConfig(seq_len=1024, hidden_size=1536, num_layers=24, num_heads=16, vocab_size=51200)  #Microbatch: 64  #GPU: 2  Parallel Config: LoadSolutionParallelArgs(prefer_reduce_scatter=True, use_remat=True, num_auto_layers=6, forward_stage_layer_ids=[[0, 1, 2], [3, 4, 5]], submesh_physical_shapes=[[1, 1], [1, 1]], submesh_logical_shapes=[[1, 1], [1, 1]], submesh_autosharding_option_dicts=[{'force_batch_dim_to_mesh_dim': 0}, {}])  Mean Time (s): 59.206  Std Time (s): 0.005  #Params (Billion): 0.759B  TFLOPs: 57.66  Peak Mem (GB): 14.489  Metadata: {'compilation_times': 'None', 'compute_cost_file_name': 'None', 'forward_stage_layer_ids': 'None', 'submesh_shapes': 'None', 'logical_mesh_shapes': 'None', 'autosharding_option_dicts': 'None'}  
running spends 5min 12s
Working on case: BenchmarkCase(batch_size=1024, model_config=GPTModelConfig(seq_len=1024, hidden_size=1024, num_layers=24, num_heads=16, vocab_size=51200), num_micro_batches=32, parallel_mode='uniform', parallel_args=UniformParallelArgs(prefer_reduce_scatter=True, use_remat=True, dp=1, op=1, pp=1, force_batch_dim_mapping=True))
 - Setup device mesh: 0.00 s
 - Prepare input: 0.07 s
 - Create train state: 5.38 s
 - Compile (driver): 14.82 s
 - Compile (workers): 122.70 s
#total: 0, #all-reduce: 0, #all-gather: 0, #reduce-scatter: 0, #all-to-all: 0
alloc_mem: 21.52 GB
Iteration 0 ...
Iteration 1 ...
Iteration 2 ...
 - Benchmark: 227.73 s
Type: gpt  Model Config: GPTModelConfig(seq_len=1024, hidden_size=1024, num_layers=24, num_heads=16, vocab_size=51200)  #Microbatch: 32  #GPU: 1  Parallel Config: UniformParallelArgs(prefer_reduce_scatter=True, use_remat=True, dp=1, op=1, pp=1, force_batch_dim_mapping=True)  Mean Time (s): 75.251  Std Time (s): 0.079  #Params (Billion): 0.355B  TFLOPs: 43.66  Peak Mem (GB): 26.098  Metadata: {'ilp_objective': '0.00'}  
Working on case: BenchmarkCase(batch_size=1024, model_config=GPTModelConfig(seq_len=1024, hidden_size=1536, num_layers=24, num_heads=16, vocab_size=51200), num_micro_batches=32, parallel_mode='uniform', parallel_args=UniformParallelArgs(prefer_reduce_scatter=True, use_remat=True, dp=1, op=2, pp=1, force_batch_dim_mapping=True))
 - Setup device mesh: 0.00 s
 - Prepare input: 0.08 s
 - Create train state: 6.20 s
 - Compile (driver): 47.70 s
 - Compile (workers): 78.30 s
#total: 148, #all-reduce: 148, #all-gather: 0, #reduce-scatter: 0, #all-to-all: 0
alloc_mem: 14.36 GB
Iteration 0 ...
Iteration 1 ...
Iteration 2 ...
 - Benchmark: 224.58 s
Type: gpt  Model Config: GPTModelConfig(seq_len=1024, hidden_size=1536, num_layers=24, num_heads=16, vocab_size=51200)  #Microbatch: 32  #GPU: 2  Parallel Config: UniformParallelArgs(prefer_reduce_scatter=True, use_remat=True, dp=1, op=2, pp=1, force_batch_dim_mapping=True)  Mean Time (s): 73.257  Std Time (s): 0.004  #Params (Billion): 0.759B  TFLOPs: 46.60  Peak Mem (GB): 19.061  Metadata: {'ilp_objective': '1469712653.49'}  
running spends 12min 27s
