import json
from typing import Dict

from datasets import Dataset
import numpy as np
import transformers

from fastchat.conversation import get_default_conv_template, SeparatorStyle


def preprocess(sources, tokenizer: transformers.PreTrainedTokenizer,
               ignore_token_id) -> Dict:
    conv = get_default_conv_template("vicuna").copy()
    roles = {"human": conv.roles[0], "gpt": conv.roles[1]}

    # Apply prompt templates
    conversations = []
    for i, source in enumerate(sources):
        if roles[source[0]["from"]] != conv.roles[0]:
            # Skip the first one if it is not from human
            source = source[1:]

        conv.messages = []
        for j, sentence in enumerate(source):
            role = roles[sentence["from"]]
            assert role == conv.roles[j % 2], f"{i}"
            conv.append_message(role, sentence["value"])
        conversations.append(conv.get_prompt())

    # Tokenize conversations
    input_ids = tokenizer(
        conversations,
        return_tensors="np",
        padding="max_length",
        max_length=tokenizer.model_max_length,
        truncation=True,
    ).input_ids
    targets = np.copy(input_ids)

    assert conv.sep_style == SeparatorStyle.ADD_COLON_TWO

    # Mask targets
    sep = conv.sep + conv.roles[1] + ": "
    for conversation, target in zip(conversations, targets):
        total_len = int((target != tokenizer.pad_token_id).sum())

        rounds = conversation.split(conv.sep2)
        cur_len = 1
        target[:cur_len] = ignore_token_id
        for i, rou in enumerate(rounds):
            if rou == "":
                break

            parts = rou.split(sep)
            if len(parts) != 2:
                break
            parts[0] += sep
            round_len = len(tokenizer(rou).input_ids)
            instruction_len = len(tokenizer(parts[0]).input_ids) - 2

            target[cur_len:cur_len + instruction_len] = ignore_token_id

            cur_len += round_len
        target[cur_len:] = ignore_token_id

        if cur_len < tokenizer.model_max_length:
            if cur_len != total_len:
                target[:] = ignore_token_id
                print(
                    f"WARNING: tokenization mismatch: {cur_len} vs. {total_len}."
                    f" (ignored)")

    return dict(
        input_ids=input_ids,
        labels=targets,
        attention_mask=np.array(input_ids != tokenizer.pad_token_id),
    )


class LazySupervisedDataset:
    """Dataset for supervised fine-tuning."""

    def __init__(self, raw_data, tokenizer: transformers.PreTrainedTokenizer,
                 ignore_token_id):
        super(LazySupervisedDataset, self).__init__()
        print("Formatting inputs...Skip in lazy mode")
        self.tokenizer = tokenizer
        self.raw_data = raw_data
        self.cached_data_dict = {}
        self.ignore_token_id = ignore_token_id

    def __len__(self):
        return len(self.raw_data)

    def __getitem__(self, i):
        if i in self.cached_data_dict:
            return self.cached_data_dict[i]

        ret = preprocess([self.raw_data[i]["conversations"]], self.tokenizer,
                         self.ignore_token_id)
        ret = dict(
            input_ids=ret["input_ids"][0],
            labels=ret["labels"][0],
            attention_mask=ret["attention_mask"][0],
        )
        self.cached_data_dict[i] = ret

        return ret

    def iter(self):

        def gen():
            for i in range(len(self)):
                yield self[i]

        return gen


def make_supervised_data_module(tokenizer: transformers.PreTrainedTokenizer,
                                data_path, ignore_token_id) -> Dict:
    """Make dataset and collator for supervised fine-tuning."""
    print("Loading data...")
    raw_data = json.load(open(data_path, "r"))

    # Split train/test
    perm = np.random.permutation(len(raw_data))
    split = int(len(perm) * 0.98)
    train_indices = perm[:split]
    eval_indices = perm[split:]
    train_raw_data = [raw_data[i] for i in train_indices]
    eval_raw_data = [raw_data[i] for i in eval_indices]
    print(f"#train {len(train_raw_data)}, #eval {len(eval_raw_data)}")

    train_dataset = LazySupervisedDataset(train_raw_data,
                                          tokenizer=tokenizer,
                                          ignore_token_id=ignore_token_id)
    eval_dataset = LazySupervisedDataset(eval_raw_data,
                                         tokenizer=tokenizer,
                                         ignore_token_id=ignore_token_id)
    train_dataset = Dataset.from_generator(train_dataset.iter())
    eval_dataset = Dataset.from_generator(eval_dataset.iter())
    return dict(train_dataset=train_dataset, eval_dataset=eval_dataset)
